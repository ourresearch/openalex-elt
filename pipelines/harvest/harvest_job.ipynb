{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9c4f39-903d-4361-8e01-824d517a02bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "# import dlt\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, StructType, StructField, BooleanType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from dataclasses import asdict, is_dataclass\n",
    "from inspect import getmembers\n",
    "from openalex_http import http_cache\n",
    "import sqlite3\n",
    "from typing import List\n",
    "from botocore.config import Config\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d915cf2-3400-49fc-b13b-4706ca1d3ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup env variables\n",
    "scope_name = \"openalex-elt\"\n",
    "secrets = dbutils.secrets.list(scope=scope_name)\n",
    "AWS_ACCESS_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "\n",
    "env = {}\n",
    "\n",
    "for secret in secrets:\n",
    "    key = secret.key\n",
    "    value = dbutils.secrets.get(scope=scope_name, key=key)\n",
    "    env[key] = value\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8d7f52-c0ae-4754-86df-c07e2950c4f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openalex_taxicab.harvest import HarvestResult\n",
    "from openalex_taxicab.legacy.harvest import PublisherLandingPageHarvester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f332ad95-6d18-43b9-bc74-f4bf5f98e374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "S3_CONFIG = Config(\n",
    "    connect_timeout=10, \n",
    "    read_timeout=20      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66030b48-28d0-4ae3-89b9-4ef1ec5b50f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "PARTITIONS = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba471098-b966-4806-a87f-f088569d5369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def harvest_single_doi(doi, plp_harvester: PublisherLandingPageHarvester) -> HarvestResult:\n",
    "    # d = HarvestResult(s3_path=None, last_harvested=None, content=None, url=None).to_dict()\n",
    "    # d['error'] = 'DUMMY'\n",
    "    # return d\n",
    "    try:\n",
    "        result: HarvestResult = plp_harvester.harvest(doi)\n",
    "        d = result.to_dict()\n",
    "        d['error'] = None\n",
    "        d['last_harvested'] = d['last_harvested_dt']\n",
    "        for key in {'content', 'last_harvested_dt'}:\n",
    "            if key in d:\n",
    "                del d[key]\n",
    "        return d\n",
    "    except Exception as e:\n",
    "        d = HarvestResult(s3_path=None, last_harvested=None, content=None, url=None).to_dict()\n",
    "        d['error'] = str(e)\n",
    "        print(f'Error harvesting {doi}: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def harvest(dois: List[str]):\n",
    "    s3 = boto3.client('s3', \n",
    "                aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                aws_secret_access_key=AWS_SECRET_KEY,\n",
    "                config=S3_CONFIG)\n",
    "    http_cache.initialize(env)\n",
    "    plp_harvester = PublisherLandingPageHarvester(s3)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=len(dois)) as executor:\n",
    "        future_to_doi = {executor.submit(harvest_single_doi, doi, plp_harvester): doi for doi in dois}\n",
    "        results = {}\n",
    "        for future in as_completed(future_to_doi):\n",
    "            doi = future_to_doi[future]\n",
    "            result = future.result()\n",
    "            results[doi] = result\n",
    "        \n",
    "        return results\n",
    "\n",
    "harvest_result_schema = StructType([\n",
    "    StructField(\"s3_path\", StringType(), True),\n",
    "    StructField(\"last_harvested\", TimestampType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"code\", IntegerType(), True),  # Optional field\n",
    "    StructField(\"elapsed\", FloatType(), True),  # Optional field\n",
    "    StructField(\"resolved_url\", StringType(), True),\n",
    "    StructField(\"content_type\", StringType(), True),\n",
    "    StructField(\"is_soft_block\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "harvest_udf = F.udf(harvest, MapType(StringType(), harvest_result_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41ae21-8d16-47a2-b5ba-909bb3103779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crossref_df = spark.read.format(\"delta\").table(\"crossref.crossref_works\")\n",
    "crossref_df.display()\n",
    "crossref_df.cache().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817cb9c1-bf52-47b1-bfb7-3f097dab364f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "harvested_df = spark.read.format(\"delta\").table(\"harvest.harvest_results\")\n",
    "harvested_df_filtered = harvested_df.filter(F.col(\"url\").startswith(\"https://doi.org/\"))\n",
    "harvested_df_with_doi = harvested_df_filtered.withColumn(\"doi\", F.regexp_replace(F.col(\"url\"), \"^https://doi.org/\",\"\"))\n",
    "unharvested_df = crossref_df.join(harvested_df, [\"doi\"], \"left_anti\").select(\"doi\")\n",
    "unharvested_df = unharvested_df.limit(100000)\n",
    "unharvested_df.display()\n",
    "unharvested_df.cache().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ade3737-26ef-488b-9ba2-722a274df606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unharvested_df = unharvested_df.withColumn(\"batch_id\", F.floor(F.monotonically_increasing_id() / BATCH_SIZE))\n",
    "unharvested_batches_df = unharvested_df.groupBy(\"batch_id\").agg(F.collect_list(\"doi\").alias(\"doi_list\"))\n",
    "\n",
    "\n",
    "unharvested_batches_df = unharvested_batches_df.repartition(PARTITIONS)\n",
    "unharvested_batches_df.display()\n",
    "unharvested_batches_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c3383e-b2dc-4fc5-ae25-9237c484ffac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "harvest_result_df = unharvested_batches_df.withColumn('harvest_results', harvest_udf(unharvested_batches_df['doi_list'])).select('harvest_results')\n",
    "\n",
    "# processed_batches = 0\n",
    "# processed_dois = 0\n",
    "# start_time = time.time()\n",
    "# total_dois = unharvested_df.cache().count()\n",
    "\n",
    "# for batch in harvest_result_df.toLocalIterator():\n",
    "#     processed_batches += 1\n",
    "#     processed_dois += len(batch['harvest_results'])\n",
    "#     current_time = time.time()\n",
    "#     elapsed_time = current_time - start_time\n",
    "#     speed = processed_dois / (elapsed_time / 3600) if elapsed_time > 0 else 0\n",
    " \n",
    "#     print(f\"Processed: {processed_dois}/{total_dois} | Speed: {speed:.2f} DOIs/hour | Elapsed time: {elapsed_time:.2f} seconds\", flush=True)\n",
    "# print('Finished harvesting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7631e399-64e5-402a-88d4-a13313b0f440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "harvest_result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b441c96c-ee9b-4ac4-be3f-ffabe9c03700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exploded_df = harvest_result_df.select(\n",
    "    F.explode(\"harvest_results\").alias(\"doi\", \"harvest_result\")\n",
    ")f\n",
    "final_df = exploded_df.select(\n",
    "    \"doi\",\n",
    "    F.col(\"harvest_result.*\")\n",
    ")\n",
    "final_df.display()\n",
    "final_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bf88c5-bb82-4236-852b-05dd9e7ecd24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"harvest.harvest_results\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "harvest_job",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
