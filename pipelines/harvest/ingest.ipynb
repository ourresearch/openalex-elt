{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aed6c00-bcfb-4fef-8576-527557445f80",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip uninstall openalex_taxicab -y\n",
    "%pip install git+https://github.com/ourresearch/openalex-taxicab.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9c4f39-903d-4361-8e01-824d517a02bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import dlt\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, StructType, StructField, BooleanType, TimestampType, IntegerType, FloatType\n",
    "from dataclasses import asdict, is_dataclass\n",
    "from inspect import getmembers\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d915cf2-3400-49fc-b13b-4706ca1d3ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup env variables\n",
    "scope_name = \"openalex-elt\"\n",
    "secrets = dbutils.secrets.list(scope=scope_name)\n",
    "AWS_ACCESS_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "\n",
    "\n",
    "env = {}\n",
    "\n",
    "for secret in secrets:\n",
    "    key = secret.key\n",
    "    value = dbutils.secrets.get(scope=scope_name, key=key)\n",
    "    env[key] = value\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8d7f52-c0ae-4754-86df-c07e2950c4f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openalex_taxicab.harvest import HarvestResult\n",
    "from openalex_taxicab.legacy.harvest import PublisherLandingPageHarvester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f332ad95-6d18-43b9-bc74-f4bf5f98e374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_SECRET_ACCESS_KEY')\n",
    "LOCAL_S3_DB_LOOKUP_PATH = '/tmp/s3_lookup.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba471098-b966-4806-a87f-f088569d5369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def explode_dict_column(df, dict_column_name, schema=None):\n",
    "    if df.rdd.isEmpty():\n",
    "        return df\n",
    "    sample_dict = df.select(dict_column_name).filter(F.col(dict_column_name).isNotNull()).first()\n",
    "    if not sample_dict:\n",
    "        return df\n",
    "\n",
    "    keys = sample_dict[0].keys()\n",
    "    select_exprs = [F.when(F.col(dict_column_name).isNotNull(), F.col(dict_column_name)[key]).alias(key) for key in keys]\n",
    "    exploded_df = df.select('*', *select_exprs).drop(dict_column_name)\n",
    "    \n",
    "    if schema and isinstance(schema, StructType):\n",
    "        exploded_df = exploded_df.select(schema.fieldNames())\n",
    "    \n",
    "    return exploded_df\n",
    "\n",
    "\n",
    "def harvest(doi):\n",
    "    try:\n",
    "        from openalex_http import http_cache\n",
    "        from openalex_taxicab.legacy.s3_cache import download_s3_lookup_db\n",
    "        s3 = boto3.client('s3', \n",
    "                    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                    aws_secret_access_key=AWS_SECRET_KEY)\n",
    "        if not os.path.exists(LOCAL_S3_DB_LOOKUP_PATH):\n",
    "            s3_lookup_temp_path = download_s3_lookup_db(s3)\n",
    "            os.rename(s3_lookup_temp_path, LOCAL_S3_DB_LOOKUP_PATH)\n",
    "        else:\n",
    "            s3_lookup_temp_path = LOCAL_S3_DB_LOOKUP_PATH \n",
    "        http_cache.initialize(env)\n",
    "        db_conn = sqlite3.connect(s3_lookup_temp_path)\n",
    "        plp_harvester = PublisherLandingPageHarvester(s3, db_conn)\n",
    "        result: HarvestResult = plp_harvester.harvest(doi)\n",
    "        d = result.to_dict()\n",
    "        d['error'] = None\n",
    "        for key in {'content', 'last_harvested_dt'}:\n",
    "            if key in d:\n",
    "                del d[key]\n",
    "        return d\n",
    "    except Exception as e:\n",
    "        d = HarvestResult(s3_path=None, last_harvested=None, content=None, url=None).to_dict()\n",
    "        d['error'] = str(e)\n",
    "        print(f'Error harvesting {doi}: {e}')\n",
    "        return None\n",
    "\n",
    "harvest_udf = F.udf(harvest, MapType(StringType(), StringType()))\n",
    "\n",
    "harvest_result_schema = StructType([\n",
    "    StructField(\"s3_path\", StringType(), True),\n",
    "    StructField(\"last_harvested\", TimestampType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"code\", IntegerType(), True),  # Optional field\n",
    "    StructField(\"elapsed\", FloatType(), True),  # Optional field\n",
    "    StructField(\"resolved_url\", StringType(), True),\n",
    "    StructField(\"content_type\", StringType(), True),\n",
    "    StructField(\"is_soft_block\", BooleanType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41ae21-8d16-47a2-b5ba-909bb3103779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def harvested_content():\n",
    "#     crossref_df = spark.read.format(\"delta\").load(\"dbfs:/pipelines/063e402f-4261-43ca-8952-8bdc31aa3d48/tables/crossref_works\").limit(100)\n",
    "#     dois_df = crossref_df.select('DOI')\n",
    "#     harvest_result_df = dois_df.withColumn('harvest_result', harvest_udf(dois_df['DOI']))\n",
    "#     exploded_df = explode_dict_column(harvest_result_df, 'harvest_result')\n",
    "#     return exploded_df\n",
    "# harvested_content().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "551d8baa-74b0-441c-a8fe-63e92cf1a2cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"harvested_content\",\n",
    "    comment=\"Metadata about harvested URLs (S3 path, response code, etc)\",\n",
    "    table_properties={'quality': 'bronze'}\n",
    ")\n",
    "def harvested_content():\n",
    "    crossref_df = dlt.read(\"crossref_works\")\n",
    "    dois_df = crossref_df.select('DOI').repartition(30)\n",
    "    harvest_result_df = dois_df.withColumn('harvest_result', harvest_udf(dois_df['DOI']))\n",
    "    exploded_df = explode_dict_column(harvest_result_df, 'harvest_result')\n",
    "    return exploded_df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
