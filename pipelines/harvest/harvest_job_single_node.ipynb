{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80751d6f-9476-416a-9266-cb86f566ae08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall openalex_taxicab -y\n",
    "%pip uninstall openalex_http -y\n",
    "%pip install git+https://github.com/ourresearch/openalex-taxicab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9c4f39-903d-4361-8e01-824d517a02bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "# import dlt\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, StructType, StructField, BooleanType, TimestampType, IntegerType, FloatType, ArrayType\n",
    "from dataclasses import asdict, is_dataclass\n",
    "from inspect import getmembers\n",
    "from openalex_http import http_cache\n",
    "import sqlite3\n",
    "from typing import List, Dict, Any\n",
    "from botocore.config import Config\n",
    "import time\n",
    "from tenacity import RetryError\n",
    "import logging\n",
    "from threading import Thread, Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED\n",
    "from multiprocessing import Manager, Value, Lock\n",
    "import warnings\n",
    "import signal\n",
    "from functools import partial\n",
    "from urllib3.exceptions import InsecureRequestWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369d2947-b901-4475-8459-afcf450fa861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger('openalex_http')\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# Suppress only the InsecureRequestWarning from urllib3\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d915cf2-3400-49fc-b13b-4706ca1d3ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup env variables\n",
    "scope_name = \"openalex-elt\"\n",
    "secrets = dbutils.secrets.list(scope=scope_name)\n",
    "AWS_ACCESS_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "\n",
    "env = {}\n",
    "\n",
    "for secret in secrets:\n",
    "    key = secret.key\n",
    "    value = dbutils.secrets.get(scope=scope_name, key=key)\n",
    "    env[key] = value\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8d7f52-c0ae-4754-86df-c07e2950c4f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openalex_taxicab.harvest import HarvestResult\n",
    "from openalex_taxicab.legacy.harvest import PublisherLandingPageHarvester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f332ad95-6d18-43b9-bc74-f4bf5f98e374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = dbutils.secrets.get(scope=scope_name, key='AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "S3_CONFIG = Config(\n",
    "    connect_timeout=10, \n",
    "    read_timeout=20,\n",
    "    signature_version='s3v4'      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66030b48-28d0-4ae3-89b9-4ef1ec5b50f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "NUM_PROCESSES = 28\n",
    "NUM_THREADS_PER_PROCESS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9e33d5-4a97-49e9-9237-a1f30a1b8a98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "HARVEST_TIMEOUT = 60\n",
    "\n",
    "def harvest_single_doi(doi, plp_harvester: PublisherLandingPageHarvester) -> Dict[str, Any]:\n",
    "    try:\n",
    "        result = plp_harvester.harvest(doi)\n",
    "        d = result.to_dict()\n",
    "        d['error'] = None\n",
    "        d['last_harvested'] = d['last_harvested_dt']\n",
    "        for key in {'content', 'last_harvested_dt'}:\n",
    "            if key in d:\n",
    "                del d[key]\n",
    "    except Exception as e:\n",
    "        d = HarvestResult(s3_path=None, last_harvested=None, content=None, url=None).to_dict()\n",
    "        d['error'] = str(e)\n",
    "    return d\n",
    "\n",
    "def worker_process(dois, aws_access_key, aws_secret_key, s3_config, num_threads, progress_dict):\n",
    "    s3 = boto3.client('s3', \n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_key,\n",
    "                config=s3_config)\n",
    "    http_cache.initialize(env)\n",
    "    plp_harvester = PublisherLandingPageHarvester(s3)\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        future_to_doi = {executor.submit(harvest_single_doi, doi, plp_harvester): doi for doi in dois}\n",
    "        for future in as_completed(future_to_doi):\n",
    "            doi = future_to_doi[future]\n",
    "            try:\n",
    "                result = future.result(timeout=HARVEST_TIMEOUT)\n",
    "            except TimeoutError:\n",
    "                error_msg = f\"Timeout: DOI harvest exceeded {HARVEST_TIMEOUT} seconds\"\n",
    "                result = HarvestResult(s3_path=None, last_harvested=None, content=None, url=None).to_dict()\n",
    "                result['error'] = error_msg\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Unexpected error: {str(e)}\"\n",
    "                result = HarvestResult(s3_path=None, last_harvested=None, content=None, url=None).to_dict()\n",
    "                result['error'] = error_msg\n",
    "\n",
    "            with progress_dict['lock']:\n",
    "                progress_dict['processed'] += 1\n",
    "                if result['error'] is not None:\n",
    "                    progress_dict['errors'] += 1\n",
    "                    progress_dict['last_error'] = f\"DOI {doi}: {result['error']}\"\n",
    "            \n",
    "            results.append((doi, result))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def harvest(dois: List[str], num_processes: int, num_threads: int):\n",
    "    manager = Manager()\n",
    "    progress_dict = manager.dict()\n",
    "    progress_dict['processed'] = 0\n",
    "    progress_dict['errors'] = 0\n",
    "    progress_dict['last_error'] = ''\n",
    "    progress_dict['lock'] = manager.Lock()\n",
    "\n",
    "    start = time.time()\n",
    "    total_dois = len(dois)\n",
    "\n",
    "    def log_progress():\n",
    "        while progress_dict['processed'] < total_dois:\n",
    "            current_processed = progress_dict['processed']\n",
    "            current_errors = progress_dict['errors']\n",
    "            elapsed_time = time.time() - start\n",
    "            speed = current_processed / (elapsed_time / 3600) if elapsed_time > 0 else 0\n",
    "            error_pct = (current_errors/current_processed)*100 if current_processed > 0 else 0\n",
    "            last_error = progress_dict['last_error']\n",
    "            \n",
    "            print(f\"Processed: {current_processed}/{total_dois} | \"\n",
    "                  f\"Speed: {speed:.2f} DOIs/hour | \"\n",
    "                  f\"Errors: {error_pct:.2f}% | \"\n",
    "                  f\"Elapsed: {elapsed_time:.2f} seconds | \"\n",
    "                  f\"Last Error: {last_error}\", flush=True)\n",
    "            \n",
    "            time.sleep(5)\n",
    "\n",
    "    log_thread = Thread(target=log_progress)\n",
    "    log_thread.start()\n",
    "\n",
    "    chunk_size = len(dois) // num_processes\n",
    "    doi_chunks = [dois[i:i + chunk_size] for i in range(0, len(dois), chunk_size)]\n",
    "\n",
    "    worker_func = partial(worker_process, \n",
    "                          aws_access_key=AWS_ACCESS_KEY, \n",
    "                          aws_secret_key=AWS_SECRET_KEY, \n",
    "                          s3_config=S3_CONFIG, \n",
    "                          num_threads=num_threads, \n",
    "                          progress_dict=progress_dict)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        all_results = list(executor.map(worker_func, doi_chunks))\n",
    "\n",
    "    log_thread.join()\n",
    "\n",
    "    # Flatten results\n",
    "    results = {doi: result for chunk in all_results for doi, result in chunk}\n",
    "    return results\n",
    "\n",
    "harvest_result_schema = StructType([\n",
    "    StructField(\"doi\", StringType(), True),\n",
    "    StructField(\"s3_path\", StringType(), True),\n",
    "    StructField(\"last_harvested\", TimestampType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"code\", IntegerType(), True),  # Optional field\n",
    "    StructField(\"elapsed\", FloatType(), True),  # Optional field\n",
    "    StructField(\"resolved_url\", StringType(), True),\n",
    "    StructField(\"content_type\", StringType(), True),\n",
    "    StructField(\"is_soft_block\", BooleanType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41ae21-8d16-47a2-b5ba-909bb3103779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crossref_df = spark.read.format(\"delta\").table(\"crossref.crossref_works\")\n",
    "crossref_df.display()\n",
    "crossref_df.cache().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817cb9c1-bf52-47b1-bfb7-3f097dab364f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "harvested_df = spark.read.format(\"delta\").table(\"harvest.harvest_results\")\n",
    "harvested_df_filtered = harvested_df.filter(F.col(\"url\").startswith(\"https://doi.org/\"))\n",
    "harvested_df_with_doi = harvested_df_filtered.withColumn(\"doi\", F.regexp_replace(F.col(\"url\"), \"^https://doi.org/\",\"\"))\n",
    "unharvested_df = crossref_df.join(harvested_df, [\"doi\"], \"left_anti\").select(\"doi\")\n",
    "# unharvested_df = unharvested_df.orderBy(F.rand()).limit(100000)\n",
    "unharvested_df.display()\n",
    "unharvested_df.cache().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c3383e-b2dc-4fc5-ae25-9237c484ffac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "doi_list = [row['doi'] for row in unharvested_df.collect()]\n",
    "results = harvest(doi_list, NUM_PROCESSES, NUM_THREADS_PER_PROCESS)\n",
    "\n",
    "harvested_data = []\n",
    "for doi, result in results.items():\n",
    "    row = {'doi': doi}\n",
    "    row.update(result)\n",
    "    harvested_data.append(row)\n",
    "\n",
    "harvest_result_df = spark.createDataFrame(harvested_data, schema=harvest_result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b441c96c-ee9b-4ac4-be3f-ffabe9c03700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# harvest_result_df = harvest_result_df.select(\n",
    "#     F.explode(\"harvest_results\").alias(\"doi\", \"harvest_result\")\n",
    "# )\n",
    "# harvest_result_df = harvest_result_df.select(\n",
    "#     \"doi\",\n",
    "#     F.col(\"harvest_result.*\")\n",
    "# )\n",
    "harvest_result_df.display()\n",
    "harvest_result_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bf88c5-bb82-4236-852b-05dd9e7ecd24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "harvest_result_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"harvest.harvest_results\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "harvest_job_single_node",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
