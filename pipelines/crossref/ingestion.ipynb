{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossref Data Ingestion\n",
    "\n",
    "This notebook handles the ingestion of raw data from the Crossref API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Crossref API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_crossref_data(cursor=None):\n",
    "    url = \"https://api.crossref.org/works\"\n",
    "    params = {\n",
    "        \"filter\": \"from-index-date:2024-09-23,until-index-date:2024-09-23\",\n",
    "        \"rows\": 1000,\n",
    "        \"cursor\": cursor if cursor else \"*\",\n",
    "        \"sort\": \"indexed\",\n",
    "        \"order\": \"desc\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DLT Table for Raw Crossref Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    comment=\"Raw Crossref works data\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def crossref_raw_data():\n",
    "    def fetch_all_data():\n",
    "        cursor = None\n",
    "        page = 1\n",
    "        total_rows = 0\n",
    "        \n",
    "        while True:\n",
    "            data = fetch_recent_crossref_data(cursor)\n",
    "            items = data.get(\"message\", {}).get(\"items\", [])\n",
    "            rows_this_page = len(items)\n",
    "            \n",
    "            for item in items:\n",
    "                yield {'doi': item['DOI'], 'message': json.dumps(item)}\n",
    "            \n",
    "            total_rows += rows_this_page\n",
    "            logger.info(f\"Fetched page: {page} | Rows this page: {rows_this_page} | Total rows: {total_rows} | Cursor: {cursor}\")\n",
    "            \n",
    "            page += 1\n",
    "            new_cursor = data.get(\"message\", {}).get(\"next-cursor\")\n",
    "            if not new_cursor or new_cursor == cursor:\n",
    "                logger.info(f\"Pagination complete. Total pages: {page-1} | Total rows: {total_rows}\")\n",
    "                break\n",
    "            cursor = new_cursor\n",
    "\n",
    "    raw_data_schema = StructType([\n",
    "        StructField(\"doi\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(fetch_all_data(), schema=raw_data_schema) \\\n",
    "               .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    \n",
    "    logger.info(f\"Raw data ingestion complete. Total rows: {df.count()}\")\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
